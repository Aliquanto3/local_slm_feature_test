# ğŸ§ª Wavestone Local AI Workbench

**Auteur :** AnaÃ«l YAHI (Consultant Senior IA & Data Science, Wavestone)  
**Licence :** Apache 2.0

Ce projet est une application **Streamlit** conÃ§ue pour benchmarker et dÃ©montrer les capacitÃ©s des **Small Language Models (SLM)** tournant localement sur CPU, et les comparer avec les modÃ¨les Cloud de l'API Mistral AI (Large, Small, Ministral).

L'objectif est de prouver la viabilitÃ© de l'IA GÃ©nÃ©rative "Edge" (offline) pour des cas d'usage mÃ©tiers spÃ©cifiques (Triage, RGPD, RAG, IoT) sans nÃ©cessiter de GPU coÃ»teux.

![Workbench Screenshot](https://via.placeholder.com/800x400?text=Wavestone+Local+AI+Workbench+Preview)

## ğŸš€ FonctionnalitÃ©s

* **Moteur Hybride :** Basculez instantanÃ©ment entre InfÃ©rence Locale (CPU via `llama.cpp`) et InfÃ©rence Cloud (API Mistral).
* **ModÃ¨les SupportÃ©s :**
    * ğŸ  **Local :** Llama 3.2 (1B/3B), Qwen 2.5, Gemma 2, Phi-3.5, SmolLM2.
    * â˜ï¸ **API :** Mistral Large 3, Mistral Small 3.2, Magistral (Reasoning), Ministral 3 (3B/8B/14B).
* **Cas d'Usage IntÃ©grÃ©s :**
    * ğŸ¢ **Ops :** Triage d'emails et Anonymisation RGPD.
    * ğŸ¤– **IoT :** Simulation de commandes via Function Calling.
    * ğŸ“ **RAG :** SynthÃ¨se de documents PDF/TXT.
    * ğŸ’» **Code & Logique :** GÃ©nÃ©ration de code et Chain of Thought.
* **Gestion Intelligente :** TÃ©lÃ©chargement automatique des modÃ¨les GGUF et gestion dynamique de la RAM (cache clearing).

## ğŸ› ï¸ PrÃ©requis Techniques

Pour reproduire cet environnement (spÃ©cifiquement sous Windows), il est **impÃ©ratif** de respecter la version de Python ci-dessous pour Ã©viter les erreurs de compilation C++.

* **OS :** Windows 10/11 (TestÃ©) ou Linux/Mac.
* **Python :** **Version 3.11** (Requis pour la compatibilitÃ© des roues prÃ©-compilÃ©es `llama-cpp-python`).
* **MatÃ©riel :** CPU (8GB+ RAM recommandÃ©). Pas de GPU nÃ©cessaire.

## ğŸ“¦ Installation

### 1. Cloner le projet
```bash
git clone [https://github.com/votre-username/wavestone-local-ai-workbench.git](https://github.com/votre-username/wavestone-local-ai-workbench.git)
cd wavestone-local-ai-workbench
```

### 2. CrÃ©er l'environnement virtuel (Python 3.11)
Assurez-vous d'avoir Python 3.11 installÃ©.
```powershell
# Windows (PowerShell)
py -3.11 -m venv .venv
```

### 3. Installer les dÃ©pendances
C'est l'Ã©tape critique. Nous installons une version prÃ©-compilÃ©e de `llama-cpp-python` pour CPU pour Ã©viter d'avoir Ã  installer Visual Studio Build Tools.

```powershell
# 1. Installer llama-cpp-python (Version CPU prÃ©-compilÃ©e pour Windows/3.11)
.\.venv\Scripts\python.exe -m pip install llama-cpp-python --extra-index-url [https://abetlen.github.io/llama-cpp-python/whl/cpu](https://abetlen.github.io/llama-cpp-python/whl/cpu)

# 2. Installer le reste des dÃ©pendances (Streamlit, Mistral, etc.)
.\.venv\Scripts\python.exe -m pip install -r requirements.txt
```

## â¬‡ï¸ TÃ©lÃ©chargement des ModÃ¨les

Le projet inclut un script utilitaire qui tÃ©lÃ©charge automatiquement les versions quantifiÃ©es (GGUF Q4_K_M) optimisÃ©es pour CPU.

```powershell
.\.venv\Scripts\python.exe download_gguf_models.py
```
*Le script vÃ©rifiera l'existence des fichiers dans le dossier `models_gguf/` et ne tÃ©lÃ©chargera que les manquants.*

## âš™ï¸ Configuration API (Optionnel)

Pour utiliser les modÃ¨les **Cloud** (Mistral Large, Ministral API, Magistral), vous avez besoin d'une clÃ© API Mistral.

1.  Obtenez une clÃ© sur [console.mistral.ai](https://console.mistral.ai/).
2.  Deux mÃ©thodes pour l'utiliser :
    * **Directement dans l'interface :** Entrez la clÃ© dans la barre latÃ©rale de l'application.
    * **Variable d'environnement :** DÃ©finissez `MISTRAL_API_KEY` dans votre systÃ¨me.

## â–¶ï¸ Lancement de l'Application

```powershell
.\.venv\Scripts\python.exe -m streamlit run app.py
```
L'application s'ouvrira automatiquement dans votre navigateur Ã  l'adresse `http://localhost:8501`.

## ğŸ› DÃ©pannage Courant

**Erreur : `Failed to load model from file` / `tensor not found`**
* Vous utilisez probablement une version obsolÃ¨te de `llama-cpp-python`. Assurez-vous d'avoir installÃ© la version via la commande indiquÃ©e ci-dessus (Ã©tape 3).
* VÃ©rifiez que le fichier GGUF a bien Ã©tÃ© tÃ©lÃ©chargÃ© complÃ¨tement (taille > 500Mo).

**Erreur : `ModuleNotFoundError: No module named 'streamlit'`**
* Vous n'utilisez pas l'exÃ©cutable python de votre environnement virtuel. Utilisez bien `.\.venv\Scripts\python.exe ...`.

**Lenteur extrÃªme :**
* C'est normal sur CPU pour les gros modÃ¨les (>7B). PrÃ©fÃ©rez les modÃ¨les "Tiny" (Llama 3.2 1B, Gemma 2 2B, Qwen 2.5 1.5B) pour une expÃ©rience fluide sur PC portable standard.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Merci d'ouvrir une issue avant de proposer une PR majeure.

## ğŸ“œ Licence

Ce projet est sous licence **Apache 2.0**.
Copyright Â© 2025 Wavestone.