# ğŸ§ª Wavestone Local AI Workbench

**Auteur :** [AnaÃ«l YAHI](https://www.linkedin.com/in/anaÃ«l-yahi/) (Consultant Senior IA & Data Science, Wavestone)  
**Licence :** [Apache 2.0](https://fr.wikipedia.org/wiki/Licence_Apache) 
*(cette application est libre et open source, si elle inclut les mentions de copyright)*

Ce projet est une application **Streamlit** conÃ§ue pour benchmarker et dÃ©montrer les capacitÃ©s des **Small Language Models (SLM)** tournant localement sur CPU, et les comparer avec les modÃ¨les Cloud de l'API Mistral AI (Large, Small, Ministral).

L'objectif est de prouver la viabilitÃ© de l'IA GÃ©nÃ©rative "Edge" (offline) pour des cas d'usage mÃ©tiers spÃ©cifiques (Triage, RGPD, RAG, IoT) sans nÃ©cessiter de GPU coÃ»teux.

![Workbench Screenshot](https://raw.githubusercontent.com/Aliquanto3/local_slm_feature_test/refs/heads/main/documentation/workbench_screenshot.png)

## ğŸš€ FonctionnalitÃ©s

* **Moteur Hybride :** Basculez instantanÃ©ment entre InfÃ©rence Locale (CPU via `llama.cpp`) et InfÃ©rence Cloud (API Mistral).
* **ModÃ¨les SupportÃ©s :**
    * ğŸ  **Local :** 
        * [granite-4.0-350m](https://huggingface.co/ibm-granite/granite-4.0-350m)
        * [granite-4.0-1b](https://huggingface.co/ibm-granite/granite-4.0-1b)
        * [granite-3.0-3b-a800m-instruct](https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct)
        * [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)
        * [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)
        * [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
        * [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)
        * [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
        * [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
        * [SmolLM2-1.7B-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)     
    * â˜ï¸ **API :** Mistral Large 3, Mistral Small 3.2, Magistral (Reasoning), Ministral 3 (3B/8B/14B).
        * [Mistral Large 3](https://docs.mistral.ai/models/mistral-large-3-25-12)
        * [Mistral Small 3.2](https://docs.mistral.ai/models/mistral-small-3-2-25-06) 
        * [Magistral Small 1.2](https://docs.mistral.ai/models/magistral-small-1-2-25-09)
        * [Ministral 3 14B](https://docs.mistral.ai/models/ministral-3-14b-25-12)
        * [Ministral 3 8B](https://docs.mistral.ai/models/ministral-3-8b-25-12)
        * [Ministral 3 3B](https://docs.mistral.ai/models/ministral-3-3b-25-12)
* **Cas d'Usage IntÃ©grÃ©s :**
    * ğŸ¢ **Ops :** Triage d'emails et Anonymisation RGPD.
    * ğŸ¤– **IoT :** Simulation de commandes via Function Calling.
    * ğŸ“ **RAG :** SynthÃ¨se de documents PDF/TXT.
    * ğŸ’» **Code & Logique :** GÃ©nÃ©ration de code et Chain of Thought.
* **Gestion Intelligente :** TÃ©lÃ©chargement automatique des modÃ¨les GGUF et gestion dynamique de la RAM (cache clearing).

## ğŸ› ï¸ PrÃ©requis Techniques

Pour reproduire cet environnement (spÃ©cifiquement sous Windows), il est **impÃ©ratif** de respecter la version de Python ci-dessous pour Ã©viter les erreurs de compilation C++.

* **OS :** Windows 10/11 *(testÃ© sur 11)* ou Linux/Mac.
* **Python :** **Version 3.11** (Requis pour la compatibilitÃ© des roues prÃ©-compilÃ©es `llama-cpp-python`).
* **MatÃ©riel :** CPU (8GB+ RAM recommandÃ©). Pas de GPU nÃ©cessaire.

## ğŸ“¦ Installation

### 1. Cloner le projet
```bash
git clone [https://github.com/Aliquanto3/local_slm_feature_test](https://github.com/Aliquanto3/local_slm_feature_test)
cd local_slm_feature_test
```

### 2. CrÃ©er l'environnement virtuel (Python 3.11)
Assurez-vous d'avoir [Python 3.11](https://www.python.org/downloads/release/python-3119/) installÃ© *(testÃ© sur 3.11.9)*.
```powershell
# Windows (PowerShell)
py -3.11 -m venv .venv
```

### 3. Installer les dÃ©pendances
C'est l'Ã©tape critique. Nous installons une version prÃ©-compilÃ©e de `llama-cpp-python` pour CPU pour Ã©viter d'avoir Ã  installer Visual Studio Build Tools.

```powershell
# 1. Installer llama-cpp-python (Version CPU prÃ©-compilÃ©e pour Windows/3.11)
.\.venv\Scripts\python.exe -m pip install "https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.2/llama_cpp_python-0.3.2-cp311-cp311-win_amd64.whl" --force-reinstall --no-cache-dir
```

```powershell
# 2. Installer le reste des dÃ©pendances (Streamlit, Mistral, etc.)
.\.venv\Scripts\python.exe -m pip install -r requirements.txt
```
*__Remarque__ : il est proposÃ© ici d'utiliser ".\.venv\Scripts\python.exe" plutÃ´t que de passer complÃ¨tement dans le venv pour pouvoir exÃ©cuter ce code sur les PC bloquant l'exÃ©cution de scripts.*

## â¬‡ï¸ TÃ©lÃ©chargement des ModÃ¨les

Le projet inclut un script utilitaire qui tÃ©lÃ©charge automatiquement les versions quantifiÃ©es (GGUF Q4_K_M) optimisÃ©es pour CPU.

```powershell
.\.venv\Scripts\python.exe download_gguf_models.py
```
*Le script vÃ©rifiera l'existence des fichiers dans le dossier `models_gguf/` et ne tÃ©lÃ©chargera que les manquants.*

__Remarque__ : avant de commencer les tÃ©lÃ©chargements, vous pouvez essayer le paramÃ¨tre -dry-run pour vÃ©rifier que toutes les sources fonctionnent.

```powershell
.\.venv\Scripts\python.exe download_gguf_models.py --dry-run
```

Le paramÃ¨tre --force vous permet de rÃ©installer des modÃ¨les dÃ©jÃ  existants (par exemple si vous craignez que le fichier initial soit corrompu).

```powershell
.\.venv\Scripts\python.exe download_gguf_models.py --force
```

## âš™ï¸ Configuration API (Optionnel)

Pour utiliser les modÃ¨les **Cloud** (Mistral Large, Ministral API, Magistral), vous avez besoin d'une clÃ© API Mistral.

1.  Obtenez une clÃ© sur [console.mistral.ai](https://console.mistral.ai/).
2.  Deux mÃ©thodes pour l'utiliser :
    * **Directement dans l'interface :** Entrez la clÃ© dans la barre latÃ©rale de l'application.
    * **Variable d'environnement :** DÃ©finissez `MISTRAL_API_KEY` dans votre systÃ¨me.

## â–¶ï¸ Lancement de l'Application

```powershell
.\.venv\Scripts\python.exe -m streamlit run app.py
```
L'application s'ouvrira automatiquement dans votre navigateur Ã  l'adresse `http://localhost:8501`.

## ğŸ˜Š Test de l'application
Tout est correctement installÃ© et fonctionnel ? Regardez le fichier [Test Protocol](https://github.com/Aliquanto3/local_slm_feature_test/blob/main/documentation/TEST_PROTOCOL.md) pour des idÃ©es de fonctionnalitÃ©s Ã  tester !

## ğŸ’¡ Et pour aller plus loin ?
Vous souhaitez essayer d'autres modÃ¨les ? 
Modifiez directement le JSON du fichier [models_config.py](https://github.com/Aliquanto3/local_slm_feature_test/blob/main/config/models_config.py) pour y intÃ©grer les caractÃ©ristiques du modÃ¨le de votre choix. Si vous remplissez correctement le JSON, vous pourrez alors tÃ©lÃ©charger le modÃ¨le via le script de tÃ©lÃ©chargement, puis le voir s'afficher directement dans l'application.
*__Remarque__ : Assurez-vous de trouver un lien de tÃ©lÃ©chargement pour un modÃ¨le "GGUF", pour qu'il soit compatible avec la libraire "llama-cpp-python" utilisÃ©e pour l'infÃ©rence locale.*

## ğŸ› DÃ©pannage Courant

**Erreur : `Failed to load model from file` / `tensor not found`**
* Vous utilisez probablement une version obsolÃ¨te de `llama-cpp-python`. Assurez-vous d'avoir installÃ© la version via la commande indiquÃ©e ci-dessus (Ã©tape 3).
* VÃ©rifiez que le fichier GGUF a bien Ã©tÃ© tÃ©lÃ©chargÃ© complÃ¨tement (taille > 500Mo).

**Erreur : `ModuleNotFoundError: No module named 'streamlit'`**
* Vous n'utilisez pas l'exÃ©cutable python de votre environnement virtuel. Utilisez bien `.\.venv\Scripts\python.exe ...`.

**Lenteur extrÃªme :**
* C'est normal sur CPU pour les gros modÃ¨les (>7B). PrÃ©fÃ©rez les modÃ¨les "Tiny" (Llama 3.2 1B, Gemma 2 2B, Qwen 2.5 1.5B) pour une expÃ©rience fluide sur PC portable standard.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Merci d'ouvrir une issue avant de proposer une PR majeure.

## ğŸ“œ Licence

Ce projet est sous licence **Apache 2.0**.
Copyright Â© 2025 [Wavestone](https://www.wavestone.com/fr/).
Veuillez crÃ©diter [AnaÃ«l Yahi](https://www.linkedin.com/in/anaÃ«l-yahi/) lors de la rÃ©utilisation de ce projet.