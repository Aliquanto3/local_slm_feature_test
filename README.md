# ğŸ¤– Wavestone Local AI Workbench

Une application **Streamlit** conÃ§ue pour benchmarker, dÃ©montrer et expÃ©rimenter avec des "Small Language Models" (SLM) directement en local (CPU/GPU), sans connexion internet ni envoi de donnÃ©es vers le cloud.

![Python](https://img.shields.io/badge/Python-3.11%2B-blue)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![Models](https://img.shields.io/badge/Models-Granite%20%7C%20Ministral%20%7C%20Llama%20%7C%20Qwen-green)

## ğŸ“‚ Structure du Projet

Voici l'organisation des fichiers telle que configurÃ©e :

```text
ğŸ“ wavestone-local-ai/
â”œâ”€â”€ ğŸ“„ app.py                  # L'application principale (Interface Streamlit)
â”œâ”€â”€ ğŸ“„ setup_models.py         # Script d'automatisation des tÃ©lÃ©chargements (HuggingFace)
â”œâ”€â”€ ğŸ“„ requirements.txt        # Liste des dÃ©pendances Python
â”œâ”€â”€ ğŸ“„ TEST_PROTOCOL.md        # ScÃ©narios de test pour les dÃ©mos clients
â”œâ”€â”€ ğŸ“„ README.md               # Documentation du projet (ce fichier)
â””â”€â”€ ğŸ“¦ *.gguf                  # Les modÃ¨les quantifiÃ©s (stockÃ©s Ã  la racine)
```

## ğŸ—ï¸ Les ModÃ¨les IntÃ©grÃ©s

L'application est configurÃ©e pour "hot-swapper" (changer Ã  la volÃ©e) entre les modÃ¨les suivants, prÃ©sents Ã  la racine :

| Famille | Fichier GGUF | Cas d'Usage PrivilÃ©giÃ© |
| :--- | :--- | :--- |
| **IBM Granite** | `granite-4.0-1b-Q4_K_M.gguf` | **Triage JSON strict, Anonymisation** (Ops Entreprise) |
| **IBM Granite** | `granite-4.0-350m-Q4_K_M.gguf` | Version ultra-lÃ©gÃ¨re pour tests rapides |
| **Mistral AI** | `Ministral-3-3B-Instruct...` | **RÃ©daction, SynthÃ¨se**, Culture FranÃ§aise |
| **Mistral AI** | `Ministral-3-3B-Reasoning...` | **Logique complexe**, ChaÃ®ne de pensÃ©e (CoT) |
| **Meta Llama** | `Llama-3.3-1B-Instruct...` | **Polyvalent**, Function Calling (IoT) |
| **Alibaba Qwen** | `qwen2.5-3b-instruct...` | **MathÃ©matiques**, Code complexe |

## ğŸš€ Installation & DÃ©marrage

### 1. PrÃ©requis
Assurez-vous d'avoir Python 3.10+ installÃ©.

```bash
# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
# Activer l'environnement (Windows)
.\venv\Scripts\activate
```

### 2. Installation des dÃ©pendances
```bash
pip install -r requirements.txt
```
*Si `requirements.txt` n'existe pas encore, installez manuellement :*
```bash
pip install streamlit llama-cpp-python huggingface_hub
```

### 3. VÃ©rification des modÃ¨les
Si vous n'avez pas encore tous les fichiers `.gguf` listÃ©s ci-dessus, lancez le script de setup :
```bash
python setup_models.py
```
> âš ï¸ **Important pour Ministral :** Ces modÃ¨les sont "Gated". Si le tÃ©lÃ©chargement Ã©choue, loguez-vous avec `huggingface-cli login` aprÃ¨s avoir acceptÃ© la licence sur le site Hugging Face.

### 4. Lancer le Workbench
```bash
streamlit run app.py
```

## ğŸ§ª Guide des DÃ©monstrations (Onglets)

L'application est divisÃ©e en pÃ´les de compÃ©tences pour simuler des cas rÃ©els Wavestone :

1.  **ğŸ¢ Ops Entreprise :**
    * *Triage d'Emails :* Le modÃ¨le analyse un email et retourne un JSON `{catÃ©gorie, urgence, sentiment}`.
    * *Anonymisation :* Nettoyage automatique des noms et emails (RGPD).
2.  **ğŸ¤– IoT & JSON :**
    * DÃ©monstration "Agentique" oÃ¹ le modÃ¨le transforme une phrase ("Allume la clim") en commande technique JSON.
3.  **ğŸ“ SynthÃ¨se & RÃ©dac :**
    * Inclut la feature "Micro-Summarization" pour gÃ©nÃ©rer des objets de mails ou des rÃ©sumÃ©s en 10 mots.
4.  **ğŸ’» Code :**
    * GÃ©nÃ©ration de Python/SQL propre, sans texte superflu (prompt systÃ¨me strict).
5.  **ğŸ§  Labo Logique :**
    * Utilise `Ministral Reasoning` pour montrer le processus de pensÃ©e interne ("Thinking process") avant de rÃ©pondre.

---
*Projet interne pour Ã©valuation des SLM.*