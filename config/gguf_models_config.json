{
  "models": [
    {
      "name": "Granite 4.0 350M GGUF",
      "repo_id": "ibm-granite/granite-4.0-350m-GGUF",
      "filename": "granite-4.0-350m-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 0.3,
      "original_model": "ibm-granite/granite-4.0-350m"
    },
    {
      "name": "Granite 4.0 1B GGUF",
      "repo_id": "ibm-granite/granite-4.0-1b-GGUF",
      "filename": "granite-4.0-1b-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 1.0,
      "original_model": "ibm-granite/granite-4.0-1b"
    },
    {
      "name": "Granite 3.0 3B A800M Instruct GGUF",
      "repo_id": "bartowski/granite-3.0-3b-a800m-instruct-GGUF",
      "filename": "granite-3.0-3b-a800m-instruct-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 2.0,
      "original_model": "ibm-granite/granite-3.0-3b-a800m-instruct"
    },
    {
      "name": "Gemma 2 2B IT GGUF",
      "repo_id": "bartowski/gemma-2-2b-it-GGUF",
      "filename": "gemma-2-2b-it-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 1.5,
      "original_model": "google/gemma-2-2b-it"
    },
    {
      "name": "Llama 3.2 1B Instruct GGUF",
      "repo_id": "bartowski/Llama-3.2-1B-Instruct-GGUF",
      "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 0.8,
      "original_model": "meta-llama/Llama-3.2-1B-Instruct"
    },
    {
      "name": "Llama 3.2 3B Instruct GGUF",
      "repo_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 2.0,
      "original_model": "meta-llama/Llama-3.2-3B-Instruct"
    },
    {
      "name": "Phi-3.5 Mini Instruct GGUF",
      "repo_id": "bartowski/Phi-3.5-mini-instruct-GGUF",
      "filename": "Phi-3.5-mini-instruct-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 2.4,
      "original_model": "microsoft/Phi-3.5-mini-instruct"
    },
    {
      "name": "Qwen2.5 1.5B Instruct GGUF",
      "repo_id": "Qwen/Qwen2.5-1.5B-Instruct-GGUF",
      "filename": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 1.0,
      "original_model": "Qwen/Qwen2.5-1.5B-Instruct"
    },
    {
      "name": "Qwen2.5 3B Instruct GGUF",
      "repo_id": "Qwen/Qwen2.5-3B-Instruct-GGUF",
      "filename": "qwen2.5-3b-instruct-q4_k_m.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 2.0,
      "original_model": "Qwen/Qwen2.5-3B-Instruct"
    },
    {
      "name": "SmolLM2 1.7B Instruct GGUF",
      "repo_id": "bartowski/SmolLM2-1.7B-Instruct-GGUF",
      "filename": "SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_gb": 1.1,
      "original_model": "HuggingFaceTB/SmolLM2-1.7B-Instruct"
    }
  ],
  "quantization_info": {
    "Q4_K_M": {
      "description": "4-bit quantization, bon compromis qualité/taille",
      "use_case": "Recommandé pour usage général sur CPU"
    },
    "Q5_K_M": {
      "description": "5-bit quantization, meilleure qualité",
      "use_case": "Si vous avez plus de RAM disponible"
    },
    "Q6_K": {
      "description": "6-bit quantization, excellente qualité",
      "use_case": "Proche de la qualité originale"
    },
    "Q8_0": {
      "description": "8-bit quantization, qualité maximale",
      "use_case": "Si RAM n'est pas une contrainte"
    }
  },
  "download_settings": {
    "local_dir": "./models_gguf",
    "resume_download": true,
    "max_workers": 2
  }
}