"""
Configuration centralis√©e des mod√®les pour Wavestone Local AI Workbench.
Fait le lien entre l'interface Streamlit et les fichiers GGUF t√©l√©charg√©s.
"""
import os

LOCAL_MODEL_DIR = "models_gguf"

MODELS_DB = {
    # -------------------------------------------------------------------------
    # ‚òÅÔ∏è MISTRAL API
    # -------------------------------------------------------------------------
    "‚òÅÔ∏è Mistral": {
        "Mistral Large 3": {
            "type": "api",
            "api_id": "mistral-large-latest",
            "ctx": 256000,
            "info": {
                "fam": "Mistral Large", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le g√©n√©raliste multimodal SOTA (texte + vision), MoE 41B/675B, "
                    "avec contexte 256K. Tr√®s adapt√© aux assistants quotidiens haut de gamme, "
                    "au RAG longue port√©e (rapports, bases documentaires), √† l‚Äôagentique "
                    "(tool calling, workflows) et aux cas d‚Äôusage d‚Äôentreprise exigeants "
                    "o√π la robustesse et la qualit√© priment sur le co√ªt."
                ),
                "params_tot": 675, "params_act": 41, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/mistral-large-3-25-12"
            }
        },
        "Mistral Small 3.2": {
            "type": "api",
            "api_id": "mistral-small-latest",
            "ctx": 128000,
            "info": {
                "fam": "Mistral Small", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le 24B multimodal (texte + vision) optimis√© pour couvrir ~80 % des "
                    "cas d‚Äôusage g√©n√©riques : chat, r√©daction, r√©sum√©, RAG, requ√™tes m√©tier. "
                    "Long contexte (128K), tr√®s bon en suivi d‚Äôinstructions, r√©duction des "
                    "r√©p√©titions et function calling. Id√©al comme 'daily driver' cloud "
                    "performant mais plus √©conomique que Large 3."
                ),
                "params_tot": 24, "params_act": 24, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/mistral-small-3-2-25-06"
            }
        },
        "Magistral Small 1.2": {
            "type": "api",
            "api_id": "magistral-small-latest",
            "ctx": 128000,
            "info": {
                "fam": "Magistral", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le 24B orient√© 'reasoning' (System 2) multimodal, d√©riv√© de Mistral "
                    "Small 3.2 avec traces de raisonnement (<think>) et entra√Ænement "
                    "suppl√©mentaire sur des t√¢ches complexes. Particuli√®rement adapt√© aux "
                    "maths, au code, aux probl√®mes STEM et aux cha√Ænes de raisonnement "
                    "explicites, tout en restant utilisable comme assistant g√©n√©raliste."
                ),
                "params_tot": 24, "params_act": 24, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/magistral-small-1-2-25-09"
            }
        },
        "Ministral 3 14B": {
            "type": "api",
            "api_id": "ministral-14b-latest",
            "ctx": 256000,
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Plus grand mod√®le dense de la famille edge (texte + vision, contexte 256K). "
                    "Offre des performances proches de Mistral Small 3.2 tout en restant "
                    "con√ßu pour le d√©ploiement local. Pertinent pour un assistant local "
                    "multimodal 'haut de gamme', du RAG avanc√©, du code et des cas m√©tier "
                    "demandant une bonne robustesse, avec une ou plusieurs GPUs."
                ),
                "params_tot": 14, "params_act": 14, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/ministral-3-14b-25-12"
            }
        },
        "Ministral 3 8B": {
            "type": "api",
            "api_id": "ministral-8b-latest",
            "ctx": 256000,
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le edge 8B multimodal, puissant et efficace, pens√© pour tourner "
                    "localement (peut tenir dans ~12 Go de VRAM en FP8, moins en quantis√©). "
                    "Tr√®s bon compromis qualit√©/latence/co√ªt pour un assistant local, du RAG "
                    "sur documents d‚Äôentreprise, du code et des t√¢ches analytiques."
                ),
                "params_tot": 8, "params_act": 8, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/ministral-3-8b-25-12"
            }
        },
        "Ministral 3 3B": {
            "type": "api",
            "api_id": "ministral-3b-latest",
            "ctx": 256000,
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Plus petit mod√®le dense de la famille edge, multimodal (texte + vision) "
                    "et contexte long (256K). Con√ßu pour environnements tr√®s contraints "
                    "(edge devices, petits serveurs), pour de la conversation l√©g√®re, "
                    "de la classification, du r√©sum√© court, du routage et des agents simples."
                ),
                "params_tot": 3, "params_act": 3, "disk": 0.0, "ram": 0.0,
                "link": "https://docs.mistral.ai/models/ministral-3-3b-25-12"
            }
        }
    },

    # -------------------------------------------------------------------------
    # üè† IBM GRANITE
    # -------------------------------------------------------------------------
    "üè† IBM - Granite": {
        "Granite 3.0 3B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-3.0-3b-a800m-instruct-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 3.0", "editor": "IBM",
                "desc": (
                    "Mod√®le MoE 3B (~800M param√®tres actifs) orient√© entreprise, open-source. "
                    "Multilingue, bon en r√©sum√©, classification, extraction, Q&A et code. "
                    "Tr√®s adapt√© aux cas d‚Äôusage d‚Äôentreprise s√©rieux (cybers√©curit√©, "
                    "conformit√©, analyse documentaire) o√π la stabilit√© et la gouvernance "
                    "sont prioritaires."
                ),
                "params_tot": 3.3, "params_act": 0.8,
                "disk": 2.06, "ram": 6.0,
                "link": "https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct"
            }
        },
        "Granite 4.0 1B": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-4.0-1b-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 4.0", "editor": "IBM",
                "desc": (
                    "Mod√®le 'nano' 1B dense/hybride, pens√© pour le edge/on-device. "
                    "Id√©al pour des t√¢ches l√©g√®res : agents simples, extraction, "
                    "classification, routage, automatisations texte sur CPU."
                ),
                "params_tot": 1.0, "params_act": 1.0,
                "disk": 1.02, "ram": 3.0,
                "link": "https://huggingface.co/ibm-granite/granite-4.0-1b"
            }
        },
        "Granite 4.0 350M": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-4.0-350m-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 4.0", "editor": "IBM",
                "desc": (
                    "Micro-mod√®le 350M ultra-l√©ger, optimal pour classification, "
                    "d√©tection d‚Äôintention, filtrage, normalisation ou routage. "
                    "Tr√®s faible empreinte m√©moire, parfait pour environnements "
                    "extr√™mement contraints."
                ),
                "params_tot": 0.35, "params_act": 0.35,
                "disk": 0.22, "ram": 1.0,
                "link": "https://huggingface.co/ibm-granite/granite-4.0-350m"
            }
        }
    },

    # -------------------------------------------------------------------------
    # üè† META LLAMA
    # -------------------------------------------------------------------------
    "üè† Meta - Llama": {
        "Llama 3.2 1B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "Llama-3.2-1B-Instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Llama 3.2", "editor": "Meta",
                "desc": (
                    "Petit mod√®le 1B multilingue optimis√© pour le dialogue, le r√©sum√©, "
                    "le tool calling l√©ger et les applications embarqu√©es. Id√©al pour "
                    "des assistants simples, des agents l√©gers, ou du traitement local "
                    "√† faible co√ªt."
                ),
                "params_tot": 1.23, "params_act": 1.23,
                "disk": 0.81, "ram": 3.0,
                "link": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
            }
        },
        "Llama 3.2 3B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "Llama-3.2-3B-Instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Llama 3.2", "editor": "Meta",
                "desc": (
                    "Mod√®le 3B multilingue, tr√®s √©quilibr√© en g√©n√©ration, r√©sum√©, "
                    "raisonnement l√©ger et code. Excellent compromis pour un assistant "
                    "local g√©n√©raliste sur GPU ou CPU puissant."
                ),
                "params_tot": 3.21, "params_act": 3.21,
                "disk": 2.02, "ram": 6.0,
                "link": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
            }
        }
    },

    # -------------------------------------------------------------------------
    # üè† QWEN
    # -------------------------------------------------------------------------
    "üè† Alibaba - Qwen": {
        "Qwen 2.5 1.5B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "qwen2.5-1.5b-instruct-q4_k_m.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Qwen 2.5", "editor": "Alibaba",
                "desc": (
                    "Petit mod√®le 1.5B tr√®s performant en multilingue, extraction, "
                    "r√©sum√©, recherche documentaire et code. Long contexte (32K). "
                    "Id√©al pour assistants l√©gers, RAG courts et automatisation m√©tier."
                ),
                "params_tot": 1.54, "params_act": 1.31,
                "disk": 1.07, "ram": 4.0,
                "link": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct"
            }
        },
        "Qwen 2.5 3B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "qwen2.5-3b-instruct-q4_k_m.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Qwen 2.5", "editor": "Alibaba",
                "desc": (
                    "Mod√®le 3B multilingue long contexte (32K), tr√®s performant en "
                    "g√©n√©ration structur√©e, code, analyse logique et agentique. Excellent "
                    "candidat comme SLM principal pour un assistant local polyvalent."
                ),
                "params_tot": 3.09, "params_act": 2.77,
                "disk": 2.10, "ram": 7.0,
                "link": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct"
            }
        }
    },

    # -------------------------------------------------------------------------
    # üè† MICROSOFT / GOOGLE / HF
    # -------------------------------------------------------------------------
    "üè† Microsoft, Google & HF": {
        "Phi-3.5 Mini Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "Phi-3.5-mini-instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Phi-3.5", "editor": "Microsoft",
                "desc": (
                    "Mod√®le 3.8B tr√®s dense en donn√©es de raisonnement, avec contexte 128K. "
                    "Excellente qualit√© en logique, maths, explications pas-√†-pas et "
                    "programmation. Particuli√®rement adapt√© au tutorat, au RAG analytique "
                    "et aux cas d‚Äôusage √©ducatifs."
                ),
                "params_tot": 3.8, "params_act": 3.8,
                "disk": 2.39, "ram": 8.0,
                "link": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
            }
        },
        "Gemma 2 2B Instruct": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "gemma-2-2b-it-Q4_K_M.gguf"),
            "ctx": 8192,
            "info": {
                "fam": "Gemma 2", "editor": "Google",
                "desc": (
                    "Mod√®le 2B open-weight de Google (techno Gemini), tr√®s bon en r√©daction, "
                    "Q&A, code et raisonnement. Fiable et s√ªr, adapt√© aux assistants texte, "
                    "documentation technique et prototypage d‚Äôagents."
                ),
                "params_tot": 2.0, "params_act": 2.0,
                "disk": 1.71, "ram": 5.0,
                "link": "https://huggingface.co/google/gemma-2-2b-it"
            }
        },
        "SmolLM2 1.7B": {
            "type": "local",
            "file": os.path.join(LOCAL_MODEL_DIR, "SmolLM2-1.7B-Instruct-Q4_K_M.gguf"),
            "ctx": 2048,
            "info": {
                "fam": "SmolLM2", "editor": "HuggingFace",
                "desc": (
                    "Mod√®le compact 1.7B con√ßu pour tourner on-device. Bon en chat simple, "
                    "r√©√©criture, r√©sum√©, extraction et classification. Id√©al pour agents "
                    "embarqu√©s, micro-services NLP et pipelines l√©gers."
                ),
                "params_tot": 1.7, "params_act": 1.7,
                "disk": 1.06, "ram": 4.0,
                "link": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct"
            }
        }
    }
}
