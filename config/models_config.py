"""
Configuration centralis√©e (Unique Source of Truth) pour Wavestone Local AI Workbench.
Utilis√© par :
1. app.py (pour l'affichage et le chargement)
2. download_gguf_models.py (pour le t√©l√©chargement)
"""
import os

# Param√®tres globaux
LOCAL_MODEL_DIR = "models_gguf"

# Configuration du t√©l√©chargement
DOWNLOAD_SETTINGS = {
    "local_dir": LOCAL_MODEL_DIR,
    "resume_download": True,
    "max_workers": 2
}

# =========================================================================
# üß© Taxonomie des r√¥les (role_pref)
# -------------------------------------------------------------------------
# Chaque mod√®le peut d√©clarer un ou plusieurs r√¥les pr√©f√©r√©s dans info["role_pref"].
#
# - "assistant_generalist"   : Assistant de chat polyvalent (g√©n√©ration, Q&A, r√©sum√©‚Ä¶)
# - "assistant_light"        : Assistant l√©ger / r√©actif pour ressources limit√©es (CPU, petite RAM)
# - "rag"                    : Bon candidat pour du RAG (surtout si long contexte)
# - "code"                   : Bon en programmation (explications, g√©n√©ration, debug)
# - "reasoning"              : Raisonnement avanc√© / logique, cha√Ænes d‚Äôexplication
# - "math_stem"              : Math√©matiques, physique, probl√®mes quantitatifs / STEM
# - "tool_calling"           : Sp√©cialis√© ou tr√®s √† l‚Äôaise en function calling / orchestration d‚Äôoutils
# - "routing_classification" : Classification, d√©tection d‚Äôintention, filtrage, routage de requ√™tes
# - "edge_on_device"         : Pens√© pour tourn¬≠er on-device / edge / CPU faible
# - "enterprise"             : Particuli√®rement adapt√© aux cas d‚Äôusage entreprise, conformit√©, gouvernance
# - "educational_tutor"      : Tutorat, p√©dagogie, explications pas-√†-pas
#
# Exemple d‚Äôacc√®s :
#   info = MODELS_DB["üè† Alibaba"]["Qwen 2.5 1.5B Instruct"]["info"]
#   roles = info.get("role_pref", [])
# =========================================================================

MODELS_DB = {
    # =========================================================================
    # üè† ALIBABA (Qwen)
    # =========================================================================
    "üè† Alibaba - Qwen": {
        "Qwen 2.5 0.5B Instruct": {
            "type": "local",
            "repo_id": "Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            "filename": "qwen2.5-0.5b-instruct-q4_k_m.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "qwen2.5-0.5b-instruct-q4_k_m.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Qwen 2.5", "editor": "Alibaba",
                "desc": (
                    "Version 'nano' de Qwen 2.5. Mod√®le dense d‚Äôenviron 0,5B param√®tres, "
                    "incroyablement l√©ger, surprenant pour des t√¢ches simples de "
                    "classification, routage, extraction de mots-cl√©s ou chat basique "
                    "sur CPU modeste."
                ),
                "params_tot": 0.5, "params_act": 0.5,
                "disk": 0.40, "ram": 1.5,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["assistant_light", "routing_classification", "edge_on_device"],
                "link": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF"
            }
        },
        "Qwen 2.5 1.5B Instruct": {
            "type": "local",
            "repo_id": "Qwen/Qwen2.5-1.5B-Instruct-GGUF",
            "filename": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "qwen2.5-1.5b-instruct-q4_k_m.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Qwen 2.5", "editor": "Alibaba",
                "desc": (
                    "Petit mod√®le dense (‚âà1,5B) tr√®s performant en multilingue, extraction, "
                    "r√©sum√©, recherche documentaire et code. Contexte 32K. Id√©al pour des "
                    "assistants l√©gers, du RAG court, des workflows m√©tier automatis√©s ou "
                    "comme mod√®le par d√©faut sur CPU ou GPU modeste."
                ),
                "params_tot": 1.54, "params_act": 1.54,
                "disk": 1.12, "ram": 4.0,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["assistant_generalist", "rag", "code", "edge_on_device"],
                "link": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF"
            }
        },
        "Qwen 2.5 3B Instruct": {
            "type": "local",
            "repo_id": "Qwen/Qwen2.5-3B-Instruct-GGUF",
            "filename": "qwen2.5-3b-instruct-q4_k_m.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "qwen2.5-3b-instruct-q4_k_m.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Qwen 2.5", "editor": "Alibaba",
                "desc": (
                    "Mod√®le dense 3B multilingue long contexte (32K), tr√®s performant en "
                    "g√©n√©ration structur√©e, code, analyse logique et sc√©narios agentiques. "
                    "Excellent candidat comme SLM principal pour un assistant local "
                    "polyvalent sur CPU puissant ou GPU."
                ),
                "params_tot": 3.09, "params_act": 3.09,
                "disk": 1.93, "ram": 7.0,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["assistant_generalist", "rag", "code", "reasoning"],
                "link": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF"
            }
        }
    },

    # =========================================================================
    # üè† GOOGLE
    # =========================================================================
    "üè† Google - Gemma": {
        "Gemma 2 2B IT": {
            "type": "local",
            "repo_id": "bartowski/gemma-2-2b-it-GGUF",
            "filename": "gemma-2-2b-it-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "gemma-2-2b-it-Q4_K_M.gguf"),
            "ctx": 8192,
            "info": {
                "fam": "Gemma 2", "editor": "Google",
                "desc": (
                    "Mod√®le 2B open-weight de Google (famille Gemma / Gemini), tr√®s bon en "
                    "r√©daction, Q&A, code et raisonnement l√©ger, avec un contexte 8K. "
                    "Fiable et plut√¥t s√©curis√©, adapt√© aux assistants texte g√©n√©raux, √† la "
                    "documentation technique et au prototypage d‚Äôagents."
                ),
                "params_tot": 2.0, "params_act": 2.0,
                "disk": 1.71, "ram": 5.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "code"],
                "link": "https://huggingface.co/google/gemma-2-2b-it"
            }
        }
    },

    # =========================================================================
    # üè† HUGGING FACE
    # =========================================================================
    "üè† Hugging Face - SmolLM": {
        "SmolLM2 1.7B Instruct": {
            "type": "local",
            "repo_id": "bartowski/SmolLM2-1.7B-Instruct-GGUF",
            "filename": "SmolLM2-1.7B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "SmolLM2-1.7B-Instruct-Q4_K_M.gguf"),
            "ctx": 2048,
            "info": {
                "fam": "SmolLM2", "editor": "HuggingFace",
                "desc": (
                    "Mod√®le compact 1.7B con√ßu pour tourner on-device (contexte 2K). "
                    "Bon en chat simple, r√©√©criture, r√©sum√©, extraction et classification. "
                    "Id√©al pour agents embarqu√©s, micro-services NLP et pipelines l√©gers "
                    "o√π la latence prime sur la profondeur de raisonnement."
                ),
                "params_tot": 1.7, "params_act": 1.7,
                "disk": 1.06, "ram": 4.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_light", "routing_classification", "edge_on_device"],
                "link": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct"
            }
        }
    },

    # =========================================================================
    # üè† IBM GRANITE
    # =========================================================================
    "üè† IBM - Granite": {
        "Granite 3.0 3B Instruct": {
            "type": "local",
            "repo_id": "bartowski/granite-3.0-3b-a800m-instruct-GGUF",
            "filename": "granite-3.0-3b-a800m-instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-3.0-3b-a800m-instruct-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 3.0", "editor": "IBM",
                "desc": (
                    "Mod√®le MoE 3.3B (~800M param√®tres actifs) orient√© entreprise. "
                    "Multilingue, bon en r√©sum√©, classification, extraction, Q&A et code. "
                    "Tr√®s adapt√© aux cas d‚Äôusage d‚Äôentreprise s√©rieux (cybers√©curit√©, "
                    "conformit√©, analyse documentaire) o√π la stabilit√©, la gouvernance "
                    "et la tra√ßabilit√© sont prioritaires."
                ),
                "params_tot": 3.3, "params_act": 0.8,
                "disk": 2.06, "ram": 6.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "enterprise", "rag", "routing_classification"],
                "link": "https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct"
            }
        },
        "Granite 3.1 2B Instruct": {
            "type": "local",
            "repo_id": "bartowski/granite-3.1-2b-instruct-GGUF",
            "filename": "granite-3.1-2b-instruct-Q4_K_M.gguf",
            "ctx": 131072,
            "info": {
                "fam": "Granite 3.1", "editor": "IBM",
                "desc": (
                    "Granite 3.1 2B dense avec contexte √©tendu √† 128K. Meilleures "
                    "performances en instruction-following et RAG longue port√©e que la "
                    "v3.0. Int√©ressant pour analyser de longs rapports, proc√®s-verbaux ou "
                    "dossiers de conformit√© sur une seule requ√™te."
                ),
                "params_tot": 2.5, "params_act": 2.5,
                "disk": 1.55, "ram": 5.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "rag", "enterprise"],
                "link": "https://huggingface.co/ibm-granite/granite-3.1-2b-instruct"
            }
        },
        "Granite 4.0 1B": {
            "type": "local",
            "repo_id": "ibm-granite/granite-4.0-1b-GGUF",
            "filename": "granite-4.0-1b-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-4.0-1b-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 4.0", "editor": "IBM",
                "desc": (
                    "Mod√®le 'nano' 1B dense/hybride, pens√© pour le edge/on-device. "
                    "Id√©al pour des t√¢ches l√©g√®res : agents simples, extraction, "
                    "classification, filtrage et routage, automatisations texte sur CPU."
                ),
                "params_tot": 1.0, "params_act": 1.0,
                "disk": 1.02, "ram": 3.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_light", "routing_classification", "edge_on_device", "enterprise"],
                "link": "https://huggingface.co/ibm-granite/granite-4.0-1b"
            }
        },
        "Granite 4.0 350M": {
            "type": "local",
            "repo_id": "ibm-granite/granite-4.0-350m-GGUF",
            "filename": "granite-4.0-350m-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "granite-4.0-350m-Q4_K_M.gguf"),
            "ctx": 4096,
            "info": {
                "fam": "Granite 4.0", "editor": "IBM",
                "desc": (
                    "Micro-mod√®le 350M (‚âà0,4B) ultra-l√©ger, optimal pour classification, "
                    "d√©tection d‚Äôintention, filtrage, normalisation ou routage. Tr√®s faible "
                    "empreinte m√©moire, parfait pour environnements extr√™mement contraints "
                    "ou comme brique de pr√©- / post-traitement."
                ),
                "params_tot": 0.4, "params_act": 0.4,
                "disk": 0.24, "ram": 1.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["routing_classification", "edge_on_device", "enterprise"],
                "link": "https://huggingface.co/ibm-granite/granite-4.0-350m"
            }
        }
    },

    # =========================================================================
    # üè† LG AI RESEARCH
    # =========================================================================
    "üè† LG AI Research - EXAONE": {
        "EXAONE 3.5 2.4B Instruct": {
            "type": "local",
            "repo_id": "bartowski/EXAONE-3.5-2.4B-Instruct-GGUF",
            "filename": "EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "EXAONE 3.5", "editor": "LG AI Research",
                "desc": (
                    "Mod√®le bilingue cor√©en/anglais 2.4B tr√®s performant de LG. "
                    "Architecture optimis√©e, long contexte 32K et bons scores sur les "
                    "benchmarks standard pour sa taille. Int√©ressant comme alternative "
                    "bilingue √† Qwen/Gemma pour des cas d‚Äôusage EN/KR."
                ),
                "params_tot": 2.4, "params_act": 2.4,
                "disk": 1.64, "ram": 5.0,
                "langs": ["ko", "en"],
                "role_pref": ["assistant_generalist", "rag"],
                "link": "https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
            }
        }
    },

    # =========================================================================
    # üè† MadeAgents
    # =========================================================================
    "üè† MadeAgents - Hammer": {
        "Hammer 2.1 0.5B": {
            "type": "local",
            "repo_id": "Nekuromento/Hammer2.1-0.5b-Q6_K-GGUF",
            "filename": "hammer2.1-0.5b-q6_k.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "hammer2.1-0.5b-q6_k.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Hammer 2.1", "editor": "MadeAgents",
                "desc": (
                    "Mod√®le ultra-compact (~0,5B) sp√©cialis√© en function calling et "
                    "pilotage d‚Äôoutils. Adapt√© aux workflows d‚Äôagents simples, √† tr√®s "
                    "faible latence, avec un contexte 32K utile pour des traces courtes "
                    "ou de petits plans d‚Äôaction."
                ),
                "params_tot": 0.5, "params_act": 0.5,
                "disk": 0.50, "ram": 2.0,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["tool_calling", "edge_on_device", "assistant_light"],
                "link": "https://huggingface.co/MadeAgents/Hammer2.1-0.5b"
            }
        },
        "Hammer 2.1 1.5B": {
            "type": "local",
            "repo_id": "mradermacher/Hammer2.1-1.5b-GGUF",
            "filename": "Hammer2.1-1.5b.Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Hammer2.1-1.5b.Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Hammer 2.1", "editor": "MadeAgents",
                "desc": (
                    "Version 1.5B de Hammer, dense, avec bon compromis taille/capacit√© "
                    "pour des agents locaux complexes orient√©s function calling "
                    "et orchestration d‚ÄôAPI."
                ),
                "params_tot": 1.5, "params_act": 1.5,
                "disk": 1.00, "ram": 4.0,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["tool_calling", "assistant_generalist"],
                "link": "https://huggingface.co/MadeAgents/Hammer2.1-1.5b"
            }
        },
        "Hammer 2.1 3B": {
            "type": "local",
            "repo_id": "mradermacher/Hammer2.1-3b-GGUF",
            "filename": "Hammer2.1-3b.Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Hammer2.1-3b.Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Hammer 2.1", "editor": "MadeAgents",
                "desc": (
                    "Le plus capable des 'petits' Hammers (~3B). Tr√®s robuste sur les "
                    "appels d'outils complexes, les plans multi-√©tapes et les sc√©narios "
                    "agentiques riches, tout en restant raisonnable en ressources."
                ),
                "params_tot": 3.0, "params_act": 3.0,
                "disk": 2.00, "ram": 6.5,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["tool_calling", "assistant_generalist", "rag"],
                "link": "https://huggingface.co/MadeAgents/Hammer2.1-3b"
            }
        }
    },

    # =========================================================================
    # üè† Meta Llama
    # =========================================================================
    "üè† Meta - Llama": {
        "Llama 3.2 1B Instruct": {
            "type": "local",
            "repo_id": "bartowski/Llama-3.2-1B-Instruct-GGUF",
            "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Llama-3.2-1B-Instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Llama 3.2", "editor": "Meta",
                "desc": (
                    "Petit mod√®le 1B multilingue optimis√© pour le dialogue, le r√©sum√©, "
                    "le tool calling l√©ger et les applications embarqu√©es, avec contexte "
                    "128K. Id√©al pour des assistants simples, des agents l√©gers ou du "
                    "traitement local √† faible co√ªt."
                ),
                "params_tot": 1.23, "params_act": 1.23,
                "disk": 0.81, "ram": 3.0,
                "langs": ["en", "fr", "de", "es", "it", "pt", "hi", "th"],
                "role_pref": ["assistant_light", "edge_on_device"],
                "link": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
            }
        },
        "Llama 3.2 3B Instruct": {
            "type": "local",
            "repo_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
            "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Llama-3.2-3B-Instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Llama 3.2", "editor": "Meta",
                "desc": (
                    "Mod√®le 3B multilingue, tr√®s √©quilibr√© en g√©n√©ration, r√©sum√©, "
                    "raisonnement l√©ger et code, avec contexte 128K. Excellent compromis "
                    "pour un assistant local g√©n√©raliste sur GPU ou CPU puissant."
                ),
                "params_tot": 3.21, "params_act": 3.21,
                "disk": 2.02, "ram": 6.0,
                "langs": ["en", "fr", "de", "es", "it", "pt", "hi", "th"],
                "role_pref": ["assistant_generalist", "rag", "code"],
                "link": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
            }
        }
    },

    # =========================================================================
    # üè† Microsoft
    # =========================================================================
    "üè† Microsoft - Phi": {
        "Phi-3.5 Mini Instruct": {
            "type": "local",
            "repo_id": "bartowski/Phi-3.5-mini-instruct-GGUF",
            "filename": "Phi-3.5-mini-instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Phi-3.5-mini-instruct-Q4_K_M.gguf"),
            "ctx": 128000,
            "info": {
                "fam": "Phi-3.5", "editor": "Microsoft",
                "desc": (
                    "Mod√®le 3.8B tr√®s dense en donn√©es de raisonnement, contexte 128K. "
                    "Excellente qualit√© en logique, maths, explications pas-√†-pas et "
                    "programmation. Particuli√®rement adapt√© au tutorat, au RAG analytique "
                    "et aux cas d‚Äôusage √©ducatifs."
                ),
                "params_tot": 3.8, "params_act": 3.8,
                "disk": 2.39, "ram": 8.0,
                "langs": ["en", "fr", "de", "es", "it", "pt", "zh"],
                "role_pref": [
                    "assistant_generalist",
                    "reasoning",
                    "math_stem",
                    "code",
                    "educational_tutor",
                    "rag"
                ],
                "link": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
            }
        }
    },

    # =========================================================================
    # üè† Nvidia
    # =========================================================================
    "üè† Nvidia - AceMath": {
        "AceMath 1.5B Instruct": {
            "type": "local",
            "repo_id": "mradermacher/AceMath-1.5B-Instruct-GGUF",
            "filename": "AceMath-1.5B-Instruct.Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "AceMath-1.5B-Instruct.Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "AceMath", "editor": "Nvidia",
                "desc": (
                    "Mod√®le sp√©cialis√© en math√©matiques et raisonnement STEM, bas√© sur "
                    "Qwen 2.5 1.5B. Tr√®s bon sur les preuves, probl√®mes quantitatifs et "
                    "explications structur√©es, avec contexte 32K."
                ),
                "params_tot": 1.54, "params_act": 1.54,
                "disk": 1.09, "ram": 4.0,
                "langs": ["en", "zh", "fr", "de", "es", "it", "pt", "ja", "ko", "ar", "ru"],
                "role_pref": ["math_stem", "reasoning", "educational_tutor"],
                "link": "https://huggingface.co/nvidia/AceMath-1.5B-Instruct"
            }
        }
    },

    # =========================================================================
    # üè† Salesforce
    # =========================================================================
    "üè† Salesforce - xLAM": {
        "xLAM-2 1B FC": {
            "type": "local",
            "repo_id": "Salesforce/xLAM-2-1b-fc-r-gguf",
            "filename": "xLAM-2-1B-fc-r-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "xLAM-2-1B-fc-r-Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "xLAM-2", "editor": "Salesforce",
                "desc": (
                    "Mod√®le 'Large Action Model' 1B sp√©cialis√© en function calling (FC). "
                    "Id√©al pour piloter des agents ou des outils API avec une tr√®s faible "
                    "latence et un contexte 32K."
                ),
                "params_tot": 1.0, "params_act": 1.0,
                "disk": 0.98, "ram": 3.5,
                "langs": ["en"],
                "role_pref": ["tool_calling", "edge_on_device"],
                "link": "https://huggingface.co/Salesforce/xLAM-2-1b-fc-r"
            }
        }
    },

    # =========================================================================
    # üè† TII UAE (Falcon)
    # =========================================================================
    "üè† TII UAE - Falcon": {
        "Falcon 3 1B Instruct": {
            "type": "local",
            "repo_id": "bartowski/Falcon3-1B-Instruct-GGUF",
            "filename": "Falcon3-1B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Falcon3-1B-Instruct-Q4_K_M.gguf"),
            "ctx": 8192,
            "info": {
                "fam": "Falcon 3", "editor": "TII UAE",
                "desc": (
                    "Nouvelle g√©n√©ration Falcon (fin 2024). Tr√®s l√©ger (1B), optimis√© pour "
                    "l'efficacit√© et le d√©ploiement edge avec contexte 8K."
                ),
                "params_tot": 1.0, "params_act": 1.0,
                "disk": 0.75, "ram": 3.0,
                "langs": ["en", "fr", "de", "es", "it", "pt", "ar"],
                "role_pref": ["assistant_light", "edge_on_device"],
                "link": "https://huggingface.co/tiiuae/Falcon3-1B-Instruct"
            }
        },
        "Falcon 3 3B Instruct": {
            "type": "local",
            "repo_id": "bartowski/Falcon3-3B-Instruct-GGUF",
            "filename": "Falcon3-3B-Instruct-Q4_K_M.gguf",
            "file": os.path.join(LOCAL_MODEL_DIR, "Falcon3-3B-Instruct-Q4_K_M.gguf"),
            "ctx": 32768,
            "info": {
                "fam": "Falcon 3", "editor": "TII UAE",
                "desc": (
                    "Grand fr√®re du 1B. Mod√®le 3B performant avec contexte 32K, "
                    "rivalisant avec Llama 3.2 3B. Bon √©quilibre vitesse/qualit√© pour un "
                    "assistant local g√©n√©raliste."
                ),
                "params_tot": 3.0, "params_act": 3.0,
                "disk": 2.01, "ram": 6.0,
                "langs": ["en", "fr", "de", "es", "it", "pt", "ar"],
                "role_pref": ["assistant_generalist"],
                "link": "https://huggingface.co/tiiuae/Falcon3-3B-Instruct"
            }
        }
    },

    # =========================================================================
    # ‚òÅÔ∏è API MISTRAL (Cloud)
    # =========================================================================
    "‚òÅÔ∏è Mistral": {
        "Mistral Large 3": {
            "type": "api",
            "api_id": "mistral-large-latest",
            "ctx": 256000,
            "eco_ops": {"kwh_1k_in": 0.0003, "kwh_1k_out": 0.0006, "embodied_g_1k": 0.12},
            "info": {
                "fam": "Mistral Large", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le g√©n√©raliste multimodal SOTA (texte + vision), MoE 675B/41B "
                    "(675B param√®tres totaux, 41B actifs), contexte 256K. "
                    "Tr√®s adapt√© aux assistants quotidiens haut de gamme, au RAG longue "
                    "port√©e (rapports, bases documentaires), √† l‚Äôagentique (tool calling, "
                    "workflows) et aux cas d‚Äôusage d‚Äôentreprise exigeants."
                ),
                "params_tot": 675, "params_act": 41,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": [
                    "assistant_generalist",
                    "rag",
                    "reasoning",
                    "code",
                    "tool_calling",
                    "enterprise"
                ],
                "link": "https://docs.mistral.ai/models/mistral-large-3-25-12"
            }
        },
        "Mistral Small 3.2": {
            "type": "api",
            "api_id": "mistral-small-latest",
            "ctx": 128000,
            "eco_ops": {"kwh_1k_in": 0.00015, "kwh_1k_out": 0.0003, "embodied_g_1k": 0.04},
            "info": {
                "fam": "Mistral Small", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le dense 24B multimodal (texte + vision) optimis√© pour couvrir "
                    "~80 % des cas d‚Äôusage g√©n√©riques : chat, r√©daction, r√©sum√©, RAG, "
                    "requ√™tes m√©tier. Contexte 128K, tr√®s bon en suivi d‚Äôinstructions et "
                    "function calling. Id√©al comme 'daily driver' cloud."
                ),
                "params_tot": 24, "params_act": 24,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "rag", "code", "tool_calling"],
                "link": "https://docs.mistral.ai/models/mistral-small-3-2-25-06"
            }
        },
        "Magistral Small 1.2": {
            "type": "api",
            "api_id": "magistral-small-latest",
            "ctx": 128000,
            "eco_ops": {"kwh_1k_in": 0.00015, "kwh_1k_out": 0.0003, "embodied_g_1k": 0.04},
            "info": {
                "fam": "Magistral", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le 24B orient√© 'reasoning' (System 2) multimodal, d√©riv√© de "
                    "Mistral Small 3.2 avec traces de raisonnement (<think>) et "
                    "entra√Ænement suppl√©mentaire sur des t√¢ches complexes. "
                    "Particuli√®rement adapt√© aux maths, au code, aux probl√®mes STEM et "
                    "aux cha√Ænes de raisonnement explicites."
                ),
                "params_tot": 24, "params_act": 24,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["reasoning", "math_stem", "code", "assistant_generalist"],
                "link": "https://docs.mistral.ai/models/magistral-small-1-2-25-09"
            }
        },
        "Ministral 3 14B": {
            "type": "api",
            "api_id": "ministral-14b-latest",
            "ctx": 256000,
            "eco_ops": {"kwh_1k_in": 0.00010, "kwh_1k_out": 0.00020, "embodied_g_1k": 0.025},
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Plus grand mod√®le dense de la famille edge (texte + vision, contexte "
                    "256K). Offre des performances proches de Mistral Small 3.2 tout en "
                    "restant con√ßu pour le d√©ploiement local. Pertinent pour un assistant "
                    "local multimodal 'haut de gamme', du RAG avanc√©, du code et des cas "
                    "m√©tier exigeants avec une ou plusieurs GPUs."
                ),
                "params_tot": 14, "params_act": 14,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "rag", "code"],
                "link": "https://docs.mistral.ai/models/ministral-3-14b-25-12"
            }
        },
        "Ministral 3 8B": {
            "type": "api",
            "api_id": "ministral-8b-latest",
            "ctx": 256000,
            "eco_ops": {"kwh_1k_in": 0.00008, "kwh_1k_out": 0.00016, "embodied_g_1k": 0.02},
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Mod√®le edge 8B multimodal, puissant et efficace, pens√© pour tourner "
                    "localement (peut tenir dans ~12 Go de VRAM en FP8, moins en quantis√©). "
                    "Tr√®s bon compromis qualit√©/latence/co√ªt pour un assistant local, du "
                    "RAG sur documents d‚Äôentreprise et des t√¢ches analytiques."
                ),
                "params_tot": 8, "params_act": 8,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_generalist", "rag", "code", "edge_on_device"],
                "link": "https://docs.mistral.ai/models/ministral-3-8b-25-12"
            }
        },
        "Ministral 3 3B": {
            "type": "api",
            "api_id": "ministral-3b-latest",
            "ctx": 256000,
            "eco_ops": {"kwh_1k_in": 0.00005, "kwh_1k_out": 0.00010, "embodied_g_1k": 0.01},
            "info": {
                "fam": "Ministral 3", "editor": "Mistral AI",
                "desc": (
                    "Plus petit mod√®le dense de la famille edge, multimodal (texte + vision) "
                    "et contexte long (256K). Con√ßu pour environnements tr√®s contraints "
                    "(edge devices, petits serveurs) pour de la conversation l√©g√®re, "
                    "de la classification, du r√©sum√© court, du routage et des agents simples."
                ),
                "params_tot": 3, "params_act": 3,
                "disk": 0.0, "ram": 0.0,
                "langs": ["en", "fr", "de", "es", "it", "pt"],
                "role_pref": ["assistant_light", "routing_classification", "edge_on_device"],
                "link": "https://docs.mistral.ai/models/ministral-3-3b-25-12"
            }
        }
    }
}

